---
layout: article
title: A note on the differences between unit testing, test first, TDD and BDD
date: 03.05.2013
author: Fabio Salvalai
published: false
---
# This article is still a draft, please consider it as such, regarding its clarity, inaccurate content, and typos.

As a manager, or as a newcomer to the art of unit testing, you will most of the time hear people around you talking about "writing tests" or "unit-testing" without any distinction regarding '''how''' they actually proceed. There are 4 major approaches into writing unit tests, and a world of difference between them.


##What does Unit test mean anyway?
Before going further into the differences between those approaches, we first need to make sure we're on the same page. You might already have a strong sense of what a unit test is, but I'll try to be concise and maybe add some meaningful nuances to your own conception of unit-testing.

###Common misconception
The word *unit-test* itself has quite a lot of meaning. Most people only consider the "test" word, and overlook the meaning of what "unit" means.
To many people, **unit**-testing means: take your big monolithic software, carve out a reasonably-sized slice out of it, consider it as a black box, and try to insufflate life into this detached part of software. Then, you will stimulate this black box and assert that what comes out it is what you expect to.

Well, guess what? You aren't testing a unit at all. Usually, when doing things this way, what happens is that the size of the black box is simply too big and too complex because of all the dependencies it requires. In fact, you would basically just be stripping the UI off your software and programmatically checking the values returned by your black box.

This practice can be useful, but those are called *Integration tests*, not unit tests. 

Additionally, some [UI-Testing frameworks](http://en.wikipedia.org/wiki/List_of_GUI_testing_tools) can even help you test your final application in real-life conditions, just by running your redistributable binaries, and test that the correct values are shown on-screen, that the navigation within your application works as expected, and that the error messages show correctly when they are supposed to.

For more details about the differences between GUI-testing, System tests, integration tests and unit tests, feel free have a look at [this dedicated article](On%20the%20differences%20between%20unit,%20integration%20and%20system%20tests.html).

###The way it is supposed to be.
Now, if you want to do it right, and write real **unit**-tests, don't think about it as "testing a slice of a monolithic software". Think in terms of "making sure the atoms which compose it are *all* doing what they are supposed to do, **one atom at a time**."

Insufflating life into a big slice of software is pretty easy. Usually, you would identify a class on the top of the food chain, one without any parameter in the constructor, or with parameters you can easily forge with *stubs*, and off you go. Problem is : since your *subject under test* is the pinnacle of the food chain, all its dependencies are real objects, with concrete implementation for their methods. That means that if your subject under test is supposed to make a file system access, a network connection, have interactions with hardware devices, or wait for an event to happen, you will have to do it in your test environment as well. This also means you will have to spend a great deal of effort into *ceremony code* to put your *subject under test* in the state in which you want to test its methods.

####Don't be intrusive
Worse even, your tests might have to change the configuration of the machine on which you are running them. For instance if you need to write a file on disk only if it doesn't exist. It would work the first time you run the tests, but next time you run it, the context of the host machine would have changed, resulting in flaky tests. Of course, you could always argue that you can clean-up after you finish your test, but there are still two problems with that. First, you will never be able to run multiple tests in parallel and leverage multi-core CPUs. The second is that, with system-intrusive tests, you never know if the state of your system at the end of a test was so because it was the purpose of the test, or if it was already in that state before. Of course, you could reset the hosting machine in a known state before running your tests, but the hosting machine was probably in that state for a reason. It seems rather obvious that you should always leave the hosting machine as you found it. If your tests are intrusive, you would then have to keep tabs on the state things were before you ran your tests, track the changes made to your system by the *subject under test*, and revert everything.You certainly don't want to do that, as it would imply you have to know all the internals of your *subject under test*. Keep in mind that what might be true one day could be refactored the day after. You definitely don't want to keep maintaining this kind of things.

###Dependency injection and Mocks.
The answer to this problem is quite simple: Use mocks and dependency injection.

####mocks
If your *subject under test* must write a file to the disk: mock the file system away.

Mocks are not to be confused with stubs. Stubs are hand-rolled implementations hard-coded by the developer to return specific values.
 
Mocks are configurable, they usually come as libraries or frameworks. My own personal favorite for the .Net language is [*moq*](http:// "http://github.com/moq"), therefore, this is what the Testeroids framework uses it under the hood, but there are many others. They all fit the same purpose, just pick the one whose syntax suits you most.
In a nutshell, a mock will allow you to create an object which will implement a given interface. the difference between stubs and mocks is that you don't need to write the class for it. The mocking framework will provide you with a way to define the behavior for each method in an interface, and make an instantiated object out of it. It will keep tabs on which methods have been called, which properties have been accessed, how many times, with which parameters, and so on. You can really think of a mocked object as a monitorable zombie object which will only reply what you taught him to reply on specific conditions, stripping it of any implementation you might expect from a real-life object. If your zombie object is being used in unknown conditions (i.e. a method call for which no configuration has been applied beforehand) the mocked object will either throw an exception or return null, depending on your mocking framework and how you configured it.

This is, by the way, why you should always respect Liskov's substitution principle and always, I mean *always* code against contracts instead of concrete classes, even though there will only ever be one single implementation for this contract. If you don't code against contracts you will not be able to use mocking correctly. You could always argue that some products like *Microsoft's Fakes*, *TypeMock* or Telerik's *JustMock pro* can get the job done, but you'll end up paying the price for both the licenses and the performances, as those advanced frameworks use very heavy profiler API's and will severely impact the performance of your tests. If implementing an interface for each and every class in your program sounds like a big overhead of work to your ear, that just means you are not using the right tools to write your code. Have a look into [ReSharper](http://www.resharper.com) if you are a .Net developer. It's free for open-source projects, and paid software for commercial use, but it deserves every penny.

####dependency injection
Now, coding against interfaces will not help you much if you new-up every object your *subject under test* depends on. The decision of new-ing up an object of a specific implementation is taken in the code of the method being tested. Therefore, the tests can never replace this object with a mock. This is where dependency injection enters the equation.
Every time you need to rely on a service, inject it. Don't use a static method, don't use a service locator, inject it!
Dependency injection is no rocket science, it's just a fancy word to say that you are passing the service you require as an argument to the constructor. This alone allow your tests to inject a mocked version of your object. From then on, you can keep tabs on it and configure it to behave the way you want it to.

##The 4 major approaches

###"Code First" unit-testing
The most naive approach about unit testing. Usually the first one you will ever put in place in your career. The problem with this approach is that it gives you absolutely no added value regarding emerging architecture or requirements specification. Basically, you are going to write your software without having testability in mind. Lots of classes will be tightly coupled, and since you didn't have to try and make an effort into mocking your dependencies, separation of concerns will probably not be your priority. Chances are, it is going to become so expensive to test your code after writing it, that you will simply give up under the pressure of a deadline. Code-first testing will never ever help you write your code ant it will obviously never speed-up the development, since all the time spent writing tests will take place after the code has been wrapped-up. It still has as least the merits of warning you in case of regressions. Some tools, like Pex, will help you generating a complete test suite which will do exactly what you don't want a unit test to do: It will write green tests, whether the logic of your code is good or flawed. Those are nice and cheap anti-regression tools, but that's all there is to it.
###"Test First" unit-testing
The Red-Green factor! The whole philosophy of test-first is about writing a test which will first fail, and for which you'll have to write the code to make it pass.
First, write the simplest, dumbest code to make the test go from red to green. If for a calculator, you write a test which tests that the addition of 3 plus 2 should equal 5, then hardcode 5 as the return value of your calculator. That's it, your job is done. If the application is obviously not doing what it should, then you might just have forgotten a test. Play dumb, really. It's not easy, because hey, we are supposed to be smart, right? The key to succeeding in test-first is to be really creative about how dumb you can get. if you need to return an instantiated class, just return new object(); do everything you can to be as anti-goal oriented in your code, and then, switch back to your tests, and be very demanding about what you want your tested method should return. If your code has been successful into making your tests go green, even if the code is absurd, don't delete that test. It's not that he's not complete enough, it's just that there's another missing: Practice '''triangulation''' : try to test what the result of your Add() method is if you pass, not 3 plus 2, but 6 plus 7. If you hardcoded 5, then it will fail. If you hardcode it to 13, then the first test you wrote will fail. Now, you have no other choice than doing the real deal and have a proper addition. If you can't find any way to play dumb any longer, then run your application and test it with real cases: you will most likely have an application that behaves the way you want to.
###Test Driven Development (TDD)
While many people which are sensibilized to the question of unit testing will agree to practice test-first, most of them will save this practice for difficult areas in the business logic, leaving the "easy" parts out. Test Driven Development, also referred to as TDD, advocates exactly the opposite. As a TDD practitioner, not only would you write the tests first, but you will do so for every part of the software, and you will literally let the tests '''drive''' both your development and design. As a matter of fact, while the last D in both TDD and BDD are commonly known as standing for Development, many have come to realize that by introducing testing at each and every level of your application, you will be forced to design your code differently to embrace some common architectural principles, like SOLID. 
Now, if you look at the cost of writing tests between code-first and test-first, you will very quickly come to the conclusion it is much faster to write the tests first. The reason for that is that you are really focused on the very topic you are trying to implement. You don't need to try and remember what it was you implemented a few days ago: you will be in the flow and it will even help you focus on the technical task you need to achieve and nothing more. Many junior developers are facing the blank page syndrome when starting on a task: they write the declaration of a method, and start typing things to see if the IDE's autocompletion will show them something they can use, without even having tried to formulate in natural language what they are trying to achieve. Writing TDD will help you doing that, but most of all, it will help you architecture things in an emergent way. You will not have to resort to complicated UML diagrams to try to figure out what the different services are and who needs which. you will simply enunciate in natural language what the required technical steps are, and inject dependencies as you need them.
###Behavior Driven Development (BDD)
BDD is quite the new shiny thing, but even though the name has only be coined lately [date needed], the concept is fairly simple. In a nutshell, take TDD, just as-is and simply change the name of the test methods. With TDD, you would always express your needs in terms of how technically the scenario unfolds, whereas, with Behavior Driven Development, you would focus more on the application's ... well, Behavior. The reason why BDD is important is because by enunciating the behavior you want to test, you will quickly see if you are doing *too much*. it is very easy for a developer to be in a coding frenzy and start doing unnecessary things, handle unnecessary scenarios, supporting low-priority edge-cases.
BDD works even better in an agile-*Scrum* environment. The rule is simple : if what you are trying to describe a scenario which is not under the umbrella of a user-story, chances are you're on your way out of scope. Stop it and get back on track.
####Another vision of BDD. 
Some frameworks, like Cucumber in ruby, or SpecFlow in .net, have a very interesting approach, where, instead of having the *product owner* [link needed] write some user stories, and let the dev team write the tests and the code for it, those frameworks would "parse" the description of the acceptance tests, and generate the test for it. The difficult part for this approach is that the acceptance tests must be very strictly formalize to follow some kind of grammar that the BDD framework will understand. I have never had the chance to work with such framework on real projects, but the idea and concept certainly are appealing. However, from my experience, a very hard part in the scrum methodology, is to get the P.O. to write acceptance tests, even if they are informal. Very often, the level of detail in just not right. Too vague, too technical, inaccurate, incomplete, undescribed edge-cases, etc. Most of the time, you can get away with it if you are using a BDD framework which doesn't rely on the parsing of the acceptance tests description, because you will just have to talk with some domain expert, or the P.O, himself, and figure things out. If you do rely on human-generated natural language, it is very likely that the strictness of the grammar that needs to be used to describe the acceptance tests will seriously impair the P.O. 
# The following paragraph should probably be refactored to an other article, linked by this one, and go in deeper details.
[When building testeroids, we chose not to be yet another Cucumber clone, but we retained a more classical approach to writing tests. For further details on this design decision, please refer to the following article : *On why Cucumber and SpecFlow are sexy utopias*]
Also, describing a simple calculator application with Cucumber of SpecFlow is fairly easy. the scenarios are not very complex, and almost all unit tests will be directly translated from the acceptance tests. While this might be true in a very simple application, things turn out differently in a complex solution where many classes, factories, repositories and services aren't even mentioned in the user-stories. Those classes will be introduced as means to an end, in order respect some basic architectural principles, such as separation of concerns and single responsibility, and improve testability. Of course, such technical services will not be described in the user-story, but they must be tested nonetheless. Therefore, how can a dev team expect to have acceptance tests describing the nitty gritty details of a user story when what is actually expected is to keep focused on the feature-oriented aspect : the P.O. describes the *What* with user stories, the dev team describes the *how* with tasks. The conclusion to this is that, in my humble opinion, it is not possible to delegate the work of writing the unit tests to the P.O. 
